\documentclass[fleqn]{hw5}

\usepackage{notes}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{xfrac} % for nice fractions?

\usepackage{verbatim} % for block comments
\usepackage[margin=0.5in]{geometry} % Override enormous margins! 

%\usepackage{xcolor,colortbl} % for filled table cells
%\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
\title{HW5: TD Learning, Q-Learning and Policy Gradient}
\duedate{Due: Mar 25, 2016}
\class{CS6300: Artificial Intelligence, Spring 2016}
\institute{University of Utah}
\author{James Watson, u1015536}
% IF YOU'RE USING THIS .TEX FILE AS A TEMPLATE, PLEASE REPLACE
% The author WITH YOUR NAME AND UID.
% Replace the due date with anyone you worked with i.e. "Worked with: John McCarthy, Watson, & Hal-9000"
\begin{document}
\maketitle

\section{Temporal Difference Learning and Q-Learning in Blockworld (70pts)}

Consider the following gridworld:
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.6\textwidth]{GridWorld}
\end{figure}

Suppose that we run two episodes that yield the
following sequences of (state, action, reward) tuples:

\begin{tabular}{ccc|ccc}
{\bf S} & {\bf A} & {\bf R} & {\bf S} & {\bf A} & {\bf R} \\
\hline
(1,1) & up    & -1     & (1,1) & up    & -1 \\
(2,1) & left  & -1     & (1,2) & up    & -1 \\
(1,1) & up    & -1     & (1,3) & right & -1 \\
(1,2) & up    & -1     & (2,3) & right & -1 \\
(1,3) & up    & -1     & (2,3) & right & -1 \\
(2,3) & right & -1     & (3,3) & right & -1 \\
(3,3) & right & -1     & (4,3) & exit  & +100 \\
(4,3) & exit  & +100   & (done)&       & \\
(done)&       &        &       &       & \\
\end{tabular}

Assume $\gamma = 1 $
\begin{enumerate}
\item \textit{According to direct estimation/evaluation, what are the values for every state
in the grid?}\\
\textbf{Answer}: The first step is to assign values to the cells encountered in each run, based on the reward at the final state
and the cost of each action. \\
\begin{tabular}{ccc|ccc}
	{\bf S} & {\bf A} & {\bf Q} & {\bf S} & {\bf A} & {\bf Q} \\
	\hline
	(1,1) & up    & +93    & (1,1) & up    & +94\\
	(2,1) & left  & +94    & (1,2) & up    & +95\\
	(1,1) & up    & +95    & (1,3) & right & +96\\
	(1,2) & up    & +96    & (2,3) & right & +97\\
	(1,3) & up    & +97    & (2,3) & right & +98\\
	(2,3) & right & +98    & (3,3) & right & +99\\
	(3,3) & right & +99    & (4,3) & exit  & +100 \\
	(4,3) & exit  & +100   & (done)&       & \\
	(done)&       &        &       &       & \\
\end{tabular}\\

Then take the average score, per state, for all states encountered in all episodes. States not yet encountered are initialized
to $0$.\\
\newcommand\B{\rule[-2ex]{0pt}{0pt}} % bottom padding for cell
\begin{figure}[h] % place the figure in sequence with text [here]

{\renewcommand{\arraystretch}{1.9}
\begin{tabular}{c|c|c|c|c|}
\hline

\textbf{3} & \vtop{\hbox{\strut $ Q(1,3) = \dfrac{(97+96)}{2}$}\hbox{\strut $\quad\quad\quad\  = 96.5 $}} \B    &
             \vtop{\hbox{\strut $ Q(2,3) = \dfrac{ (98+97+98) }{3}$}\hbox{\strut $\quad\quad\quad\  =  97.7 $}} &
             \vtop{\hbox{\strut $ Q(3,3) = \dfrac{(99+99)}{2}$}\hbox{\strut $\quad\quad\quad\  =  99 $}}        &
             \vtop{\hbox{\strut $ Q(4,3) = \dfrac{(100+100)}{2}$}\hbox{\strut $\quad\quad\quad\  =  100 $}} \B  \\ \hline 
\textbf{2} & \vtop{\hbox{\strut $ Q(1,2) = \dfrac{(96+95)}{2}$}\hbox{\strut $\quad\quad\quad\  =  95.5 $}} \B & 
             BLOCKED &
             $ Q(3,2) = 0 $ &
             $ Q(4,2) = 0^{*} $ \\ \hline
\textbf{1} & \vtop{\hbox{\strut $ Q(1,1) = \dfrac{(93+95+94)}{3.0}$}\hbox{\strut $\quad\quad\quad\  =  94 $}} \B & 
             $ Q(2,1) = 94 $ &
             $ Q(3,1) = 0 $ &
             $ Q(4,1) = 0 $ \\ \hline
           & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4}
\end{tabular}
}
\end{figure}
* (4,2) was not encountered in either episode but would have yielded a reward of $-100$

\item \textit{What are the transition probabilities for every (state, action, state) triple?}\\
\textbf{Answer}:\\
(1,1), up $\rightarrow$\\
\hspace{5 em} p(2,1): $1/3=0.33$ \\
\hspace{5 em} p(1,2): $2/3=0.67$ \\
(2,1), left $\rightarrow$\\
\hspace{5 em} p(1,1): $1/1=1.0$\\
(1,2), up $\rightarrow$\\
\hspace{5 em} p(1,3): $2/2=1.0$\\
(1,3), up $\rightarrow$\\
\hspace{5 em} p(2,3): $1/1=1.0$\\
(1,3), right $\rightarrow$\\
\hspace{5 em} p(2,3): $1/1=1.0$\\
(2,3), right $\rightarrow$\\ 
\hspace{5 em} p(2,3): $1/3=0.33$ \\
\hspace{5 em} p(3,3): $2/3=0.67$ \\
(3,3), right $\rightarrow$\\ 
\hspace{5 em} p(4,3): $2/2=1.0$ \\
(4,3), exit $\rightarrow$\\ 
\hspace{5 em} p(Goal): $2/2=1.0$ \\



\item \textit{Suppose that we run Q-learning.  We initialize all
our Q values to 10 times the max reward.  This will cause a
Q-learning agent to initially explore a lot and then eventually start
exploiting.  Why should this be true?  Justify your answer in a short
paragraph.}\\
\textbf{Answer}: As states are visited, they will be updated with $Q(s,a)\leftarrow(1-\alpha)Q(s,a)+(\alpha)[\text{sample}]$,
which will essentially reduce a visited Q-state value to $(1-\alpha)$ of its very large initial value. Thus, no visited Q-state 
can have a value as high as an unvisited Q-state, and an agent will always choose an unvisited neighboring Q-state.
\end{enumerate}

\newpage

\section{Policy Gradient (30pts)}

\textit{In order to do policy gradient, we need to be able
to compute the gradient of the evaluation function $J$ with respect
to a parameter vector ${\mathbf \theta}$: $\nabla_{\mathbf \theta} J(\theta)$.  By
our algebraic magic, we expressed this as:}

\begin{equation} \label{eq:grad}
\nabla_{\mathbf \theta} J(\mathbf \theta) 
  = \sum_a \pi_{\mathbf \theta}(s_0,a) R(a) 
             \underbrace{\nabla_{\mathbf \theta} \log \left( \pi_{\mathbf \theta}(s_0,a) \right)}_{g(s_0,a)}
\end{equation}

\textit{If we use a linear function thrown through a soft-max as our stochastic policy, we have:}

\begin{equation}
\pi_{\mathbf \theta}(s,a) 
  = \frac {\exp \left( \sum_{i=1}^n \theta_i f_i(s,a) \right)}
          {\sum_{a'} \exp \left( \sum_{i=1}^n \theta_i f_i(s,a') \right)}
\end{equation}

\textit{Compute a closed form solution for $g(s_0,a)$.  Explain in a few
sentences \emph{why} this leads to a sensible update for gradient
ascent (i.e., if we plug this in to Eq~\eqref{eq:grad} and do gradient
ascent, why is the derived form reasonable)?}\\
\textbf{Answer}:

\begin{equation}
g(s_0,a) = \nabla_{\mathbf \theta} \log \left( \frac {\exp \left( \sum_{i=1}^n \theta_i f_i(s,a) \right)}
{\sum_{a'} \exp \left( \sum_{i=1}^n \theta_i f_i(s,a') \right)} \right)
\end{equation}

\begin{equation}
g(s_0,a) = \nabla_{\mathbf \theta} \left[ \log \left( \exp \left( \sum_{i=1}^n \theta_i f_i(s,a) \right) \right)
-\log \left( \sum_{a'} \exp \left( \sum_{i=1}^n \theta_i f_i(s,a') \right) \right) \right]
\end{equation}

\begin{equation}
g(s_0,a) = \nabla_{\mathbf \theta} \left[  \sum_{i=1}^n \theta_i f_i(s,a)
-\log \left( \sum_{a'} \exp \left( \sum_{i=1}^n \theta_i f_i(s,a') \right) \right) \right]
\end{equation}

Let $ \sum_{i=1}^n \theta_i f_i(s,a') = V' $. Among $V'$, choose the largest $b_0=\max_{a'}V'$.

\begin{equation}
\log \left( \sum_{a'} \exp \left( V' \right) \right) = \log \left( \exp ( b_0 ) \sum_{a'} \exp \left( V' - b_0 \right) \right)
= b_0 + \log \left( \sum_{a'} \exp \left( V' - b_0 \right) \right)
\end{equation}
We could continue in this fashion, choosing $b_1=\max_{a'}(V'-b_0)$, but each successive term would be much smaller than the last.
Thus $\log \left( \sum_{a'} \exp \left( V' \right) \right)$ is approximated by $ \max_{a'}V' $\\
The gradient is the partial derivative with respect to each $\theta_i$, so ...

\begin{equation}
g(s_0,a) = \left[  \sum_{i=1}^n \frac{\partial}{\partial \theta_i} \theta_i f_i(s,a) \right]
-  \left[ \max_{a'} \left( \sum_{i=1}^n \frac{\partial}{\partial \theta_i} \theta_i f_i(s,a') \right) \right]
\end{equation}

\begin{equation}
g(s_0,a) = \left[  \sum_{i=1}^n f_i(s,a) \right]
-  \left[ \max_{a'} \left( \sum_{i=1}^n f_i(s,a') \right) \right]
\end{equation}

This makes sense because in the context of a policy update 
$ \theta_{k+1} = \theta_k + \alpha \sum_a \pi_{\mathbf \theta}(s_0,a) R(a) g(s_0,a) $ the neighboring state 
with the greatest value will have the greatest influence on the change in policy parameters. That is, the gradient is essentially
the difference in value between the present state and the most valuable neighboring state.


\begin{comment}
\section{Submission Instructions}

\begin{enumerate}

\item For the final submission you would be turning in a \textbf{PDF} document containing all your answers. No other submission format would be accepted. \textbf{Please ensure you write your answers using Latex or any other documenting software like MS-Word or Open Office (Linux)or Pages(MAC)} Please have your name written on top of the document. Replace the author name with your name and uID.

\item If you collaborated with a friend or classmate in doing the HW, please specify their names on the top of this document too. We hate the discussion about cheating and so please save all of us the trouble and cite appropriate sources/names.

\item Please ensure all the submissions are done through canvas. Please do not email the instructor or the TA with your submission. Submissions made via email would not be considered for grading.

\item \textbf{Naming: }Your final upload should be named in the format $<uid>$-HW5.pdf where $<uid>$ is your Utah uid. \textbf{Ex:} u0006300-HW5.pdf

\end{enumerate}
\end{comment}

\end{document}
